{"nbformat_minor": 1, "cells": [{"source": "<table style=\"border: none\" align=\"left\">\n    <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/pmservice/cars-4-you/master/static/images/logo.png\" width=\"200\" alt=\"Icon\"></th>\n       <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Business Area Prediction</b></th>\n   </tr>\n</table>", "cell_type": "markdown", "metadata": {}}, {"source": "<img align=left src=\"https://github.com/pmservice/cars-4-you/raw/master/static/images/business_area.png\" width=\"560\" alt=\"Icon\">\n", "cell_type": "markdown", "metadata": {}}, {"source": "Contents\n- [0. Setup](#setup)\n- [1. Introduction](#introduction)\n- [2. Load and explore data](#load)\n- [3. Create an Apache Spark machine learning model](#model)\n- [4. Store the model in the Watson Machine Learning repository](#persistence)\n- [5. Deploy the model in the IBM Cloud](#deployment)", "cell_type": "markdown", "metadata": {}}, {"source": "**Note:** This notebook works correctly with kernel `Python 3.5 with Spark 2.1`, please **do not change kernel**.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"setup\"></a>\n## 0. Setup\n\nIn this section please use below cell to upgrade the `watson-machine-learning-client`.", "cell_type": "markdown", "metadata": {}}, {"source": "!rm -rf $PIP_BUILD/watson-machine-learning-client\n!pip install --upgrade watson-machine-learning-client==1.0.260", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Requirement already up-to-date: watson-machine-learning-client==1.0.260 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages\nRequirement already up-to-date: tqdm in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: tabulate in /usr/local/src/conda3_runtime.v38/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: urllib3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: certifi in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: pandas in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: lomond in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: ibm-cos-sdk in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: requests in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: pytz>=2011k in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: numpy>=1.9.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: python-dateutil>=2.5.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: six>=1.10.0 in /usr/local/src/conda3_runtime.v38/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from lomond->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: ibm-cos-sdk-core==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /usr/local/src/conda3_runtime.v38/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: idna<2.8,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s081-fcdcc2c8c4a157-70f20d2e11bc/.local/lib/python3.5/site-packages (from requests->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: docutils>=0.10 in /usr/local/src/conda3_runtime.v38/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client==1.0.260)\nRequirement already up-to-date: jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v38/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client==1.0.260)\n"}], "execution_count": 1}, {"source": "**Note**: Please restart the kernel (Kernel -> Restart)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"introduction\"></a>\n## 1. Introduction\n\nThis notebook creates a spark mllib model to predict Business Area based on client feedback. The notebook shows how to train, store and deploy a model  for scoring.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"load\"></a>\n## 2. Load and explore data", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will load the data as an Apache Spark DataFrame and perform a basic exploration.", "cell_type": "markdown", "metadata": {}}, {"source": "Read data into Spark DataFrame from DB2 database and show sample record.", "cell_type": "markdown", "metadata": {}}, {"source": "**TIP:** If needed put your service credentials here.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# @hidden_cell\n# The following code is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\nproperties_db2 = {\n    'driver': 'com.ibm.db2.jcc.DB2Driver',\n    'jdbcurl': '***',\n    'user': 'dash***',\n    'password': '***'\n}\n\ntable_name = 'CAR_RENTAL_TRAINING'\ndf_data = spark.read.jdbc(properties_db2['jdbcurl'], table='.'.join([properties_db2['user'], table_name]), properties=properties_db2)\ndf_data.head()", "cell_type": "code", "metadata": {}, "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "Row(ID=74, Gender='Male', Status='M', Children=1, Age=Decimal('26.26'), Customer_Status='Active', Car_Owner='No', Customer_Service='no wait for pick up and drop off was great, help with luggage, face to face directions to hotel, recommended entertainment for area.', Satisfaction=1, Business_Area='Product: Information', Action='NA')"}, "metadata": {}}], "execution_count": 2}, {"source": "**Tip:** Code above can be inserted using Data menu.  You have to select `Insert SparkSession DataFrame` option.\n\n**Note:** Inserted code is modified to work with code in cells below.", "cell_type": "markdown", "metadata": {}}, {"source": "As you can see, the data contains eleven fields. `Business_Area` field is the one you would like to predict using feedback data in `Customer_Service` field.", "cell_type": "markdown", "metadata": {}}, {"source": "print(\"Number of records: \" + str(df_data.count()))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Number of records: 482\n"}], "execution_count": 3}, {"source": "Let's see distribution of target field.", "cell_type": "markdown", "metadata": {}}, {"source": "df_data.select('Business_Area').groupBy('Business_Area').count().show(truncate=False)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------------------------+-----+\n|Business_Area                     |count|\n+----------------------------------+-----+\n|Service: Accessibility            |26   |\n|Product: Functioning              |150  |\n|Service: Attitude                 |24   |\n|Service: Orders/Contracts         |32   |\n|Product: Availability/Variety/Size|38   |\n|Product: Pricing and Billing      |24   |\n|Product: Information              |8    |\n|Service: Knowledge                |180  |\n+----------------------------------+-----+\n\n"}], "execution_count": 4}, {"source": "<a id=\"model\"></a>\n## 3. Create an Apache Spark machine learning model\n\nIn this section you will learn how to:\n\n- [3.1 Prepare data for model training and evaluation](#prep)\n- [3.2 Create an Apache Spark machine learning pipeline](#pipe)\n- [3.3 Train a model](#train)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"prep\"></a>\n### 3.1 Prepare data for model training and evaluation\n\nIn this subsection you will split your data into: train and test data set.", "cell_type": "markdown", "metadata": {}}, {"source": "train_data, test_data = df_data.select(\"ID\", \"Customer_Service\", \"Business_Area\").randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Number of training records: 387\nNumber of testing records : 95\n"}], "execution_count": 5}, {"source": "### 3.2 Create the pipeline<a id=\"pipe\"></a>", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will create an Apache Spark machine learning pipeline and then train the model.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import StringIndexer, IndexToString, HashingTF, IDF, Tokenizer\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\nfrom pyspark.sql.types import *", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n"}], "execution_count": 6}, {"source": "In the first data preprocessing step, create features from `Customer_Service` field.", "cell_type": "markdown", "metadata": {}}, {"source": "tokenizer = Tokenizer(inputCol=\"Customer_Service\", outputCol=\"words\")\nhashing_tf = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='hash')\nidf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"features\", minDocFreq=5)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "In the following step, use the StringIndexer transformer to convert `Business_Area` to numeric.", "cell_type": "markdown", "metadata": {}}, {"source": "string_indexer_label = StringIndexer(inputCol=\"Business_Area\", outputCol=\"label\").fit(train_data)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "Add decision tree model to predict `Business_Area`.", "cell_type": "markdown", "metadata": {}}, {"source": "dt_area = DecisionTreeClassifier(labelCol=\"label\", featuresCol=idf.getOutputCol())", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 9}, {"source": "Finally, setup transformer to convert the indexed labels back to original labels.", "cell_type": "markdown", "metadata": {}}, {"source": "label_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=string_indexer_label.labels)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf, string_indexer_label, dt_area, label_converter])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 11}, {"source": "### 3.3 Train the model<a id=\"train\"></a>", "cell_type": "markdown", "metadata": {}}, {"source": "In this subsection you will train model and evaluate its accuracy.", "cell_type": "markdown", "metadata": {}}, {"source": "model = pipeline.fit(train_data)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 12}, {"source": "predictions = model.transform(test_data)\npredictions.select('Customer_Service','Business_Area','predictedLabel').show(3)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+--------------------+--------------------+\n|    Customer_Service|       Business_Area|      predictedLabel|\n+--------------------+--------------------+--------------------+\n|Initially the rep...|Product: Availabi...|  Service: Knowledge|\n|I have had a few ...|Product: Availabi...|  Service: Knowledge|\n|They did not have...|Product: Availabi...|Product: Availabi...|\n+--------------------+--------------------+--------------------+\nonly showing top 3 rows\n\n"}], "execution_count": 13}, {"source": "predictions.printSchema()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- ID: integer (nullable = true)\n |-- Customer_Service: string (nullable = true)\n |-- Business_Area: string (nullable = true)\n |-- words: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- hash: vector (nullable = true)\n |-- features: vector (nullable = true)\n |-- label: double (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = true)\n |-- predictedLabel: string (nullable = true)\n\n"}], "execution_count": 14}, {"source": "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Accuracy = %3.2f\" % accuracy)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accuracy = 0.55\n"}], "execution_count": 15}, {"source": "**Note:** Accuracy of the model is low, however based on customer comment more than one Business Area could be selected. In such cases top k (for example k=3) would be more suited for model evaluation.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"persistence\"></a>\n## 4. Store the model in the repository", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "In this section you will store trained model to Watson Machine Learning repository. When model is stored some metada is optional, however we provide it to be able to configure Continuous Learning System.", "cell_type": "markdown", "metadata": {}}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n2018-07-31 08:43:19,751 - watson_machine_learning_client.metanames - WARNING - 'AUTHOR_EMAIL' meta prop is deprecated. It will be ignored.\n"}], "execution_count": 16}, {"source": "We need Watson Machine Learning credentials to be able to store model in repository.", "cell_type": "markdown", "metadata": {}}, {"source": "**TIP:** If needed put your service credentials here.", "cell_type": "markdown", "metadata": {}}, {"source": "# @hidden_cell\n# How to get associated service credentials\n\nwml_credentials = {\n  \"apikey\": \"***\",\n  \"instance_id\": \"***\",\n  \"password\": \"***\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"***\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 17}, {"source": "client = WatsonMachineLearningAPIClient(wml_credentials)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "client.version", "cell_type": "code", "metadata": {}, "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "'1.0.260'"}, "metadata": {}}], "execution_count": 19}, {"source": "Use code in cell below to store model in Watson Machine Learning repository.", "cell_type": "markdown", "metadata": {}}, {"source": "published_model_details = client.repository.store_model(model=model, meta_props={'name':'CARS4U - Business Area Prediction Model'}, training_data=train_data, pipeline=pipeline)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 20}, {"source": "model_uid = client.repository.get_model_uid(published_model_details)\nprint(model_uid)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "d928c3e2-8eb6-4d64-892a-e3607297c89d\n"}], "execution_count": 21}, {"source": "<a id=\"deploy\"></a>\n## 5. Deploy model in the IBM Cloud", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will learn how to create model deployment in the IBM Cloud and retreive information about scoring endpoint.", "cell_type": "markdown", "metadata": {}}, {"source": "deployment_details = client.deployments.create(asset_uid=model_uid, name='CARS4U - Business Area Prediction Model Deployment')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'd928c3e2-8eb6-4d64-892a-e3607297c89d' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='4bb27d68-f2ba-4825-b451-c745450d83b4'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 22}, {"source": "You can use deployed model to score new data using scoring endpoint. You can use following command to get scoring endpoint.", "cell_type": "markdown", "metadata": {}}, {"source": "scoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "https://us-south.ml.cloud.ibm.com/v3/wml_instances/aaed6937-c0e7-4307-8a17-361aca257c7e/deployments/4bb27d68-f2ba-4825-b451-c745450d83b4/online\n"}], "execution_count": 23}, {"source": "---", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}