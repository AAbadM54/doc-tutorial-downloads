{"nbformat_minor": 1, "cells": [{"source": "<table style=\"border: none\" align=\"left\">\n    <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/pmservice/cars-4-you/master/static/images/logo.png\" width=\"200\" alt=\"Icon\"></th>\n       <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Action Recommendation</b></th>\n   </tr>\n</table>", "cell_type": "markdown", "metadata": {}}, {"source": "<img align=left src=\"https://github.com/pmservice/cars-4-you/raw/master/static/images/action.png\" width=\"550\" alt=\"Icon\">", "cell_type": "markdown", "metadata": {}}, {"source": "Contents\n- [0. Setup](#setup)\n- [1. Introduction](#introduction)\n- [2. Load and explore data](#load)\n- [3. Create an Apache Spark machine learning model](#model)\n- [4. Store the model in the Watson Machine Learning repository](#persistence)\n- [5. Deploy the model in the IBM Cloud](#persistence)\n- [6. Configure continous learning system](#learning)", "cell_type": "markdown", "metadata": {}}, {"source": "**Note:** This notebook works correctly with kernel `Python 3.5 with Spark 2.1`, please **do not change kernel**.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"setup\"></a>\n## 0. Setup\n\nIn this section please use below cell to upgrade the `watson-machine-learning-client`.", "cell_type": "markdown", "metadata": {}}, {"source": "!rm -rf $PIP_BUILD\n!pip install --upgrade watson-machine-learning-client==1.0.260", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Requirement already up-to-date: watson-machine-learning-client==1.0.260 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s861-ffd6e6d0334337-ed352cd4c457/.local/lib/python3.5/site-packages (1.0.260)\nRequirement not upgraded as not directly required: tqdm in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (4.19.4)\nRequirement not upgraded as not directly required: tabulate in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (0.8.2)\nRequirement not upgraded as not directly required: urllib3 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (1.22)\nRequirement not upgraded as not directly required: certifi in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (2018.8.24)\nRequirement not upgraded as not directly required: pandas in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (0.21.0)\nRequirement not upgraded as not directly required: lomond in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (0.1.12)\nRequirement not upgraded as not directly required: ibm-cos-sdk in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (2.0.1)\nRequirement not upgraded as not directly required: requests in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from watson-machine-learning-client==1.0.260) (2.18.4)\nRequirement not upgraded as not directly required: python-dateutil>=2 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260) (2.6.1)\nRequirement not upgraded as not directly required: pytz>=2011k in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260) (2018.4)\nRequirement not upgraded as not directly required: numpy>=1.9.0 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from pandas->watson-machine-learning-client==1.0.260) (1.13.3)\nRequirement not upgraded as not directly required: six>=1.10.0 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from lomond->watson-machine-learning-client==1.0.260) (1.11.0)\nRequirement not upgraded as not directly required: ibm-cos-sdk-core==2.*,>=2.0.0 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client==1.0.260) (2.0.1)\nRequirement not upgraded as not directly required: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk->watson-machine-learning-client==1.0.260) (2.0.1)\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->watson-machine-learning-client==1.0.260) (3.0.4)\nRequirement not upgraded as not directly required: idna<2.7,>=2.5 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from requests->watson-machine-learning-client==1.0.260) (2.6)\nRequirement not upgraded as not directly required: docutils>=0.10 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client==1.0.260) (0.14)\nRequirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v43/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk->watson-machine-learning-client==1.0.260) (0.9.3)\n\u001b[31mnotebook 5.0.0 requires nbconvert, which is not installed.\u001b[0m\n\u001b[31mipywidgets 6.0.0 requires widgetsnbextension~=2.0.0, which is not installed.\u001b[0m\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n"}], "execution_count": 1}, {"source": "**Note**: Please restart the kernel (Kernel -> Restart)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"introduction\"></a>\n## 1. Introduction\n\nThis notebook defines, trains and deploys the model that recommends specific Action for unstatisfied customers.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"load\"></a>\n## 2. Load and explore data", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will load the data as an Apache Spark DataFrame and perform a basic exploration.\nYou will also use the **bias detection** library to evaluate your data.", "cell_type": "markdown", "metadata": {}}, {"source": "Read data into Spark DataFrame from DB2 database and show sample record.", "cell_type": "markdown", "metadata": {}}, {"source": "### Load data", "cell_type": "markdown", "metadata": {}}, {"source": "**TIP:** If needed put your service credentials here.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# @hidden_cell\n# The following code is used to access your data and contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\nproperties_db2 = {\n    'driver': 'com.ibm.db2.jcc.DB2Driver',\n    'jdbcurl': '***',\n    'user': '***',\n    'password': '***'\n}\n\ntable_name = 'CAR_RENTAL_TRAINING'\ndf_data = spark.read.jdbc(properties_db2['jdbcurl'], table='.'.join([properties_db2['user'], table_name]), properties=properties_db2)\ndf_data.head()\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "error", "evalue": "An error occurred while calling o91.jdbc.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:114)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:166)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:811)\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-2990a8bc3f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtable_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CAR_RENTAL_TRAINING'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties_db2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jdbcurl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproperties_db2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties_db2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mjpredicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjpredicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o91.jdbc.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:72)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:114)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:166)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:811)\n"], "ename": "Py4JJavaError"}], "execution_count": 2}, {"source": "### Explore data", "cell_type": "markdown", "metadata": {}}, {"source": "df_data.printSchema()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Tip:** Code above can be inserted using Data menu.  You have to select `Insert SparkSession DataFrame` option.\n\n**Note:** Inserted code is modified to work with code in cells below.", "cell_type": "markdown", "metadata": {}}, {"source": "As you can see, the data contains eleven fields. `Action` field is the one you would like to predict using feedback data in `Customer_Service` field.", "cell_type": "markdown", "metadata": {}}, {"source": "print(\"Number of records: \" + str(df_data.count()))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df_data.select('Business_area').groupBy('Business_area').count().show(truncate=False)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "df_data.select('Action').groupBy('Action').count().show(truncate=False)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### Bias detection", "cell_type": "markdown", "metadata": {}}, {"source": "A data set exhibits bias if its content supports or opposes particular objects, ideologies, person groups, or beliefs in an unfair way, allowing opinions to influence unbiased judgment. If such data is used to build a machine learning model, then it is very likely that the generated model will also exhibit bias. \n\nThe code below shows you how to use the `ibm_bias_detection` library to detect potential bias in a data set about car rentals usage. For example, if you create a model to determine whether an unsatisfied customer is eligible for a voucher or with a free upgraded to rentals usage based on different criteria, you want to be sure that your training data does not exhibit bias towards any of the chosen criteria.", "cell_type": "markdown", "metadata": {}}, {"source": "Use below code to import bias detection library: `ibm_bias_detection`", "cell_type": "markdown", "metadata": {}}, {"source": "from ibm_bias_detection import data_bias_checker", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Create Pandas DataFrame.", "cell_type": "markdown", "metadata": {}}, {"source": "import pandas as pd\n\npd_data = df_data.toPandas()\npd_data.head()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "#### Input Parameters for the Bias Checker\n\nBefore you can call the function to detect bias in the data in a data set, you must characterize the bias you want to detect. You do this, by determining the input parameters to the bias checker function calls based on your sample data set.\n\nYou add the input parameters to the bias checker function into a helper map function. You can add the following parameters:\n\n - `class_label`: the name of the column in the data frame which you have designated as the classified column. This is the column whose value will be predicted by the machine learning model.\n\n This parameter is **mandatory**.\n\n Examples are a column called `Action`.\n - `protected_attributes`: an array of one or more column names which are likely to show bias towards the class-label.\n\n This parameter is **mandatory**.\n\n Examples are `Gender`, `Age`.\n - `favourable_class`: an array of one or more values of the `class_label` column which depicts a favorable outcome for the end user.\n\n This parameter is **optional**. If not specified, a library finds all of the distinct values of the `class_label` column and runs the bias detection algorithm multiple times on those extracted values. During each run it assumes one value from the set of distinct values as the favorable outcome and the rest as unfavorable. The library reports the top three biases found across all these runs.\n\n Examples of favorable outcomes for a class-label called `Action` is `On-demand pickup location`, `Voucher`, `Free Upgrade`, `Premium features`, and `NA` is an unfavorable outcome.\n - `majority`: map of expressions describing the majority group for each protected attribute.\n\n This parameter is **optional**.\n\n An example is taking `Age` as the protected attribute and specifying the majority group as `{'majority' : 'Age' : '[26,60]' } or {'majority' : 'Age' : '<60'}`.\n - `minority`: map of expressions describing the minority group for each protected attribute.\n\n This parameter is **optional**.\n\n An example is taking `Age` as the protected attribute and specifying the minority group as  `{'minority' : 'Age' : '[60,70]' } or {'minority' : 'Age' : '>=60' }`.\n - `threshold`: the decisive factor in determining the presence of bias. This value empowers organizations to have their own personalized criteria for bias.\n\n This parameter is **optional**. If not specified, the taken default is 0.8. If a bias score is below 0.8, then the presence of bias is confirmed.\n    ", "cell_type": "markdown", "metadata": {}}, {"source": "Define input parameters.", "cell_type": "markdown", "metadata": {}}, {"source": "inputs={'class_label': \"Action\",\n'protected_attributes': [\"Gender\"],\n'threshold': 0.8,\n'favourable_class': ['On-demand pickup location', 'Voucher', 'Free Upgrade', 'Premium features'],\n'source_bias': True}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Run the bias checker.", "cell_type": "markdown", "metadata": {}}, {"source": "biasD = data_bias_checker()\nbias_result = biasD.data_checker(pd_data, inputs)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "Based on the above report (no bias found) we can continue our model creation.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"model\"></a>\n## 3. Create an Apache Spark machine learning model\n\nIn this section you will learn how to:\n\n- [3.1 Prepare data for training a model](#prep)\n- [3.2 Create an Apache Spark machine learning pipeline](#pipe)\n- [3.3 Train a model](#train)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"prep\"></a>\n### 3.1 Prepare data for training a model\n\nIn this subsection you will split your data into: train and test data set.", "cell_type": "markdown", "metadata": {}}, {"source": "train_data, test_data = df_data.randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 3.2 Create the pipeline<a id=\"pipe\"></a>", "cell_type": "markdown", "metadata": {}}, {"source": "In this section you will create an Apache Spark machine learning pipeline and then train the model.", "cell_type": "markdown", "metadata": {}}, {"source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, HashingTF, IDF, Tokenizer\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "In the following step, use the StringIndexer transformer to convert all the string fields to numeric ones.", "cell_type": "markdown", "metadata": {}}, {"source": "string_indexer_gender = StringIndexer(inputCol=\"Gender\", outputCol=\"gender_ix\")\nstring_indexer_customer_status = StringIndexer(inputCol=\"Customer_Status\", outputCol=\"customer_status_ix\")\nstring_indexer_status = StringIndexer(inputCol=\"Status\", outputCol=\"status_ix\")\nstring_indexer_owner = StringIndexer(inputCol=\"Car_Owner\", outputCol=\"owner_ix\")\nstring_business_area = StringIndexer(inputCol=\"Business_Area\", outputCol=\"area_ix\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "assembler = VectorAssembler(inputCols=[\"gender_ix\", \"customer_status_ix\", \"status_ix\", \"owner_ix\", \"area_ix\", \"Children\", \"Age\", \"Satisfaction\"], outputCol=\"features\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "string_indexer_action = StringIndexer(inputCol=\"Action\", outputCol=\"label\").fit(df_data)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "label_action_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=string_indexer_action.labels)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "dt_action = DecisionTreeClassifier()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "pipeline_action = Pipeline(stages=[string_indexer_gender, string_indexer_customer_status, string_indexer_status, string_indexer_action, string_indexer_owner, string_business_area, assembler, dt_action, label_action_converter])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model_action = pipeline_action.fit(train_data)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "predictions_action = model_action.transform(test_data)\npredictions_action.select('Business_Area','Action','probability','predictedLabel').show(2)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions_action)\n\nprint(\"Accuracy = %g\" % accuracy)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "<a id=\"persistence\"></a>\n## 4. Store the model in the repository", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "In this section you will store trained model to Watson Machine Learning repository. When model is stored some metada is optional, however we provide it to be able to configure Continuous Learning System.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "We need Watson Machine Learning credentials to be able to store model in repository.", "cell_type": "markdown", "metadata": {}}, {"source": "**TIP:** If needed put your service credentials here.", "cell_type": "markdown", "metadata": {}}, {"source": "# @hidden_cell\n# How to get associated service credentials\n\nwml_credentials = {\n  \"apikey\": \"***\",\n  \"instance_id\": \"***\",\n  \"password\": \"***\",\n  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n  \"username\": \"***\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "client = WatsonMachineLearningAPIClient(wml_credentials)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "client.version", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 4.2 Save the pipeline and model<a id=\"save\"></a>", "cell_type": "markdown", "metadata": {}}, {"source": "db2_service_credentials = {\n  \"hostname\": \"***\",\n  \"password\": \"***\",\n  \"https_url\": \"***\",\n  \"port\": 50000,\n  \"ssldsn\": \"***\",\n  \"host\": \"***\",\n  \"jdbcurl\": \"***\",\n  \"uri\": \"***\",\n  \"db\": \"BLUDB\",\n  \"dsn\": \"***\",\n  \"username\": \"***\",\n  \"ssljdbcurl\": \"***\"\n}\n\ntraining_data_reference = {\n \"name\": \"CARS4U training reference\",\n \"connection\": db2_service_credentials,\n \"source\": {\n  \"tablename\": table_name,\n  \"type\": \"dashdb\"\n }\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model_props = {\n    client.repository.ModelMetaNames.NAME: \"CARS4U - Action Recommendation Model\",\n    client.repository.ModelMetaNames.TRAINING_DATA_REFERENCE: training_data_reference,\n    client.repository.ModelMetaNames.EVALUATION_METHOD: \"multiclass\",\n    client.repository.ModelMetaNames.EVALUATION_METRICS: [\n        {\n           \"name\": \"accuracy\",\n           \"value\": accuracy,\n           \"threshold\": 0.7\n        }\n    ]\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available meta names.", "cell_type": "markdown", "metadata": {}}, {"source": "published_model_details = client.repository.store_model(model=model_action, meta_props=model_props, training_data=train_data, pipeline=pipeline_action)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "model_uid = client.repository.get_model_uid(published_model_details)\nprint(model_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "<a id=\"deploy\"></a>\n## 5. Deploy model in the IBM Cloud", "cell_type": "markdown", "metadata": {}}, {"source": "You can use following command to create online deployment in cloud.", "cell_type": "markdown", "metadata": {}}, {"source": "deployment_details = client.deployments.create(model_uid=model_uid, name='CARS4U - Action Model Deployment')", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "You can use deployed model to score new data using scoring endpoint.", "cell_type": "markdown", "metadata": {}}, {"source": "scoring_url = client.deployments.get_scoring_url(deployment_details)\nprint(scoring_url)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "<a id=\"learning\"></a>\n## 6. Continuous Learning System", "cell_type": "markdown", "metadata": {}}, {"source": "### 6.1 Setup", "cell_type": "markdown", "metadata": {}}, {"source": "**TIP:** If needed put your service credentials here", "cell_type": "markdown", "metadata": {}}, {"source": "# @hidden_cell\n\nspark_credentials = {\n  \"tenant_id\": \"***\",\n  \"tenant_id_full\": \"***\",\n  \"cluster_master_url\": \"https://spark.bluemix.net\",\n  \"tenant_secret\": \"***\",\n  \"instance_id\": \"***\",\n  \"plan\": \"ibm.SparkService.PayGoPersonal\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "feedback_data_reference = {\n \"name\": \"Cars4You feedback data\",\n \"connection\": db2_service_credentials,\n \"source\": {\n  \"tablename\": \"CAR_RENTAL_FEEDBACK\",\n  \"type\": \"dashdb\"\n }\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "system_config = {\n    client.learning_system.ConfigurationMetaNames.FEEDBACK_DATA_REFERENCE: feedback_data_reference,\n    client.learning_system.ConfigurationMetaNames.MIN_FEEDBACK_DATA_SIZE: 10,\n    client.learning_system.ConfigurationMetaNames.SPARK_REFERENCE: spark_credentials,\n    client.learning_system.ConfigurationMetaNames.AUTO_RETRAIN: \"never\",\n    client.learning_system.ConfigurationMetaNames.AUTO_REDEPLOY: \"never\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "**Note:** You can update RETRAIN option to either `always` or `conditionally`. The REDEPLOY option can be also updated `always` or `conditionally`. `conditionally` means that action will happen only if new model version is better than previosly used one.", "cell_type": "markdown", "metadata": {}}, {"source": "learning_system_details = client.learning_system.setup(model_uid=model_uid, meta_props=system_config)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "### 6.2 Run learning system iteration", "cell_type": "markdown", "metadata": {}}, {"source": "run_details = client.learning_system.run(model_uid, asynchronous=False)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "client.learning_system.list()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "client.learning_system.list_runs(model_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "client.learning_system.list_metrics(model_uid)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "---", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.5 with Spark 2.1", "name": "python3-spark21", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.4", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}